# -*- coding: utf-8 -*-
"""877_Project_

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TZkkaizQyxpDUT6CAX2s2tUASsUCT-ac
"""

!pip install transformers --quiet

from transformers import pipeline, set_seed
!pip install wikipedia
!pip install praw
!pip install requests
!pip install beautifulsoup4

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import wikipedia
import praw
from transformers import pipeline, set_seed
import random
import time

# Set up Reddit API
reddit = praw.Reddit(
    client_id="dQg-iLucZAkjW4uG-PkM2g",
    client_secret="U80B7bcPGfvh5k8v2c9pToZs0BPzOw",
    user_agent="ai-human-detector"
)

# Function to fetch more Reddit posts (increase the limit)
def fetch_reddit_posts(subreddit="worldnews", limit=200):  # Increased limit
    posts = []
    for submission in reddit.subreddit(subreddit).hot(limit=limit):
        if submission.selftext and len(submission.selftext.split()) > 50:
            posts.append(submission.selftext.strip())
    return posts

# Function to fetch more paragraphs from Wikipedia (increased number of topics)
def fetch_wikipedia_paragraphs(topics=["Artificial intelligence", "Electric vehicles", "Quantum computing", "Blockchain technology", "Climate change", "Renewable energy", "Space exploration", "Machine learning"], num_sections=10):  # Increased topics and sections
    paras = []
    for topic in topics:
        try:
            content = wikipedia.page(topic).content
            splits = content.split('\n')
            filtered = [p for p in splits if len(p.split()) > 50]
            paras.extend(filtered[:num_sections])
        except:
            pass
    return paras

# Fetch the text (increased Reddit and Wikipedia)
human_reddit = fetch_reddit_posts("worldnews", 200)  # Increased limit
human_wiki = fetch_wikipedia_paragraphs(["Machine learning", "Renewable energy", "Artificial intelligence", "Climate change", "Space exploration", "Quantum computing"], 10)  # Increased number of sections
human_texts = human_reddit + human_wiki
print(f"âœ… Collected {len(human_texts)} human-written samples.")

# Set up GPT
generator = pipeline("text-generation", model="gpt2")
set_seed(42)

# Increase the number of prompts and generated AI samples
prompts = [
    "Write a short article about the future of AI.",
    "Explain the impact of social media on modern communication.",
    "Describe how electric cars are changing the world.",
    "Discuss the evolution of space travel.",
    "Explain the benefits of mental health awareness.",
    "Write a short essay on climate change and its global impact.",
    "Describe the benefits of renewable energy in modern cities.",
    "Write about the advancements in quantum computing.",
    "Explain the rise of artificial intelligence in everyday life.",
    "Discuss the role of education in combating misinformation.",
    "Discuss the environmental impacts of renewable energy.",
    "Describe the future of virtual reality in education.",
    "Write about the societal effects of automation in industries.",
    "Explain how AI can help solve climate change problems.",
    "Write about the global challenges of climate change and renewable energy adoption."
]

# Generate 50 texts per prompt (total 750 AI samples)
ai_texts = []
for prompt in prompts:
    for _ in range(50):  # Generate 50 texts per prompt
        output = generator(prompt, max_length=256, num_return_sequences=1)[0]["generated_text"]
        ai_texts.append(output)

print(f"ðŸ¤– Generated {len(ai_texts)} AI samples.")

# Combine Human and AI Texts
human_df = pd.DataFrame({'text': human_texts, 'label': 1})
ai_df = pd.DataFrame({'text': ai_texts, 'label': 0})

# Shuffle and Save
df = pd.concat([human_df, ai_df], ignore_index=True).sample(frac=1).reset_index(drop=True)
df.to_csv("/content/ai_vs_human_text_dataset.csv", index=False)
print("ðŸ“ Dataset saved to /content/ai_vs_human_text_dataset.csv")

# Check the word and character counts for the new data
def count_text_length(texts):
    total_words = sum(len(text.split()) for text in texts)
    total_chars = sum(len(text) for text in texts)
    return total_words, total_chars

# Count for human (Reddit + Wikipedia) and AI (GPT) texts
human_words, human_chars = count_text_length(human_texts)
ai_words, ai_chars = count_text_length(ai_texts)

print(f"Human text data (Reddit + Wikipedia): {human_words} words, {human_chars} characters.")
print(f"AI text data (GPT): {ai_words} words, {ai_chars} characters.")

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2").to("cuda" if torch.cuda.is_available() else "cpu")
model.eval()

# GPT2 doesn't have a pad_token by default â€” so we set it to eos_token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id

# Prompts
prompts = [
    "Write a short article about a relevent topic today.",
    "Explain the impact of social media on modern communication.",
    "Describe how electric cars are changing the world.",
    "Discuss the evolution of space travel.",
    "Explain the benefits of mental health awareness.",
    "Write a short essay on climate change and its global impact.",
    "Describe the benefits of renewable energy in modern cities.",
    "Write about the advancements in quantum computing.",
    "Explain the rise of artificial intelligence in everyday life.",
    "Discuss the role of education in combating misinformation.",
    "Discuss the environmental impacts of renewable energy.",
    "Describe the future of virtual reality in education.",
    "Write about the societal effects of automation in industries.",
    "Explain how AI can help solve climate change problems.",
    "Write about the global challenges of climate change and renewable energy adoption."
]

# Batch settings
max_length = 256
batch_size = 8

# Function to generate AI texts
def generate_ai_texts(prompts, num_per_prompt=50):
    all_outputs = []
    for prompt in prompts:
        print(f"ðŸ§  Generating for: {prompt}")
        encoded = tokenizer(prompt, return_tensors="pt", padding=True)
        input_ids = encoded['input_ids'].to(model.device)
        attention_mask = encoded['attention_mask'].to(model.device)

        for _ in range(0, num_per_prompt, batch_size):
            input_ids_batch = input_ids.repeat(batch_size, 1)
            attention_mask_batch = attention_mask.repeat(batch_size, 1)

            outputs = model.generate(
                input_ids=input_ids_batch,
                attention_mask=attention_mask_batch,
                max_length=max_length,
                do_sample=True,
                top_k=50,
                top_p=0.95,
                temperature=1.0,
                num_return_sequences=1,
                pad_token_id=tokenizer.eos_token_id  # Avoid warning
            )

            decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            all_outputs.extend(decoded)

    return all_outputs

# Generate the samples
ai_texts = generate_ai_texts(prompts, num_per_prompt=50)
print(f"ðŸ¤– Generated {len(ai_texts)} AI samples.")

import pandas as pd

# Create a DataFrame from the generated AI texts
ai_df = pd.DataFrame({'text': ai_texts})

# Save to a CSV file
ai_df.to_csv("generated_ai_texts.csv", index=False)

print("Generated AI texts have been saved to 'generated_ai_texts.csv'.")

# Function to count words and characters in the generated texts
def count_generated_data(texts):
    total_words = sum(len(text.split()) for text in texts)
    total_chars = sum(len(text) for text in texts)
    return total_words, total_chars

# Get word and character count for the generated AI texts
ai_words, ai_chars = count_generated_data(ai_texts)

# Print the results
print(f"Generated AI Text Data: {ai_words} words, {ai_chars} characters.")

#For Human data
import pandas as pd
import praw
import requests
from bs4 import BeautifulSoup

# Set up Reddit API
reddit = praw.Reddit(
    client_id="dQg-iLucZAkjW4uG-PkM2g",
    client_secret="U80B7bcPGfvh5k8v2c9pToZs0BPzOw",
    user_agent="ai-human-detector"
)

# Load AI-generated text data
ai_df = pd.read_csv('/content/generated_ai_texts.csv')
ai_texts = ai_df['text'].tolist()

# Check the word and character counts for the AI data
def count_text_length(texts):
    words = sum(len(text.split()) for text in texts)
    chars = sum(len(text) for text in texts)
    return words, chars

# Get AI text counts
ai_words, ai_chars = count_text_length(ai_texts)
print(f"Generated AI text data: {ai_words} words, {ai_chars} characters.")

# Function to fetch Reddit posts
def fetch_reddit_posts(subreddits=["worldnews", "technology", "science", "askreddit"], limit=500):
    posts = []
    for subreddit in subreddits:
        for submission in reddit.subreddit(subreddit).hot(limit=limit):
            if submission.selftext and len(submission.selftext.split()) > 50:
                posts.append(submission.selftext.strip())
    return posts

# Function to fetch news articles
def fetch_news_articles(num_articles=300):
    articles = []
    urls = [
        "https://www.bbc.com/news",
        "https://www.cnn.com/world",
        "https://www.nytimes.com"
    ]
    for url in urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, "html.parser")

         headlines = soup.find_all(['h3', 'h2'])
        for headline in headlines[:num_articles]:
            articles.append(headline.get_text())
    return articles

# Fetch human text data (Reddit + News)
additional_human_reddit = fetch_reddit_posts(subreddits=["worldnews", "technology", "askreddit"], limit=500)
additional_news_articles = fetch_news_articles(num_articles=300)

# Combine the texts
additional_human_texts = additional_human_reddit + additional_news_articles
print(f"Collected {len(additional_human_texts)} additional human-written samples.")

# Initialize the human text data
all_human_texts = additional_human_texts

# Check the current word count for human texts
human_words, human_chars = count_text_length(all_human_texts)
print(f"Human text data: {human_words} words, {human_chars} characters.")

# Continue fetching more human text data until human text data meets or exceeds AI data size
while human_words < ai_words or human_chars < ai_chars:
    print("Fetching more human text data... Current status:")
    print(f"Human text: {human_words} words, {human_chars} characters.")

    additional_human_reddit = fetch_reddit_posts(subreddits=["worldnews", "technology", "askreddit"], limit=500)
    additional_news_articles = fetch_news_articles(num_articles=300)
    additional_human_texts = additional_human_reddit + additional_news_articles
    all_human_texts += additional_human_texts  # Add to the existing human texts

    # Update word and character count for human texts
    human_words, human_chars = count_text_length(all_human_texts)
    print(f"Updated Human text data: {human_words} words, {human_chars} characters.")

    if human_words >= ai_words and human_chars >= ai_chars:
        break  # Stop once the human data meets the target size

# Combine final datasets
human_df = pd.DataFrame({'text': all_human_texts, 'label': 1})
ai_df = pd.DataFrame({'text': ai_texts, 'label': 0})

# Combine human and AI data
df = pd.concat([human_df, ai_df], ignore_index=True).sample(frac=1).reset_index(drop=True)

# Save updated dataset to a new CSV file
df.to_csv("/content/ai_vs_human_text_balanced_dataset.csv", index=False)
print("Dataset updated and saved to '/content/ai_vs_human_text_balanced_dataset.csv'.")

# Final word and character counts
human_words, human_chars = count_text_length(all_human_texts)
ai_words, ai_chars = count_text_length(ai_texts)

print(f"Human text data (Reddit + News articles): {human_words} words, {human_chars} characters.")
print(f"AI text data (GPT): {ai_words} words, {ai_chars} characters.")

!pip install beautifulsoup4 --quiet

!pip install chardet

import chardet

with open('/content/drive/MyDrive/ai_vs_human_text_balanced_dataset.csv', 'rb') as f:
    result = chardet.detect(f.read(10000))
    print(result)

import pandas as pd
import re
import string
from bs4 import BeautifulSoup

# Load the dataset

#df = pd.read_csv('/content/ai_vs_human_text_balanced_dataset.csv')
df = pd.read_csv('/content/drive/MyDrive/ai_vs_human_text_balanced_dataset.csv', encoding='latin1')




# Function to clean text
def clean_text(text):
    # Remove HTML tags
    text = BeautifulSoup(text, "html.parser").get_text()

    # Remove URLs
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

    # Remove non-printable characters and excessive whitespaces
    text = ''.join(filter(lambda x: x in string.printable, text))
    text = re.sub(r'\s+', ' ', text).strip()

    # Lowercase for BERT-uncased
    text = text.lower()

    return text

# Apply cleaning function
df['text'] = df['text'].astype(str).apply(clean_text)

# Remove empty or too-short entries
df = df[df['text'].str.split().apply(len) > 5].reset_index(drop=True)

# Save cleaned dataset
df.to_csv('/content/ai_vs_human_text_cleaned.csv', index=False)
print("Cleaned dataset saved to /content/ai_vs_human_text_cleaned.csv")
print("Sample cleaned text:\n")
print(df['text'].iloc[0][:500])

"""Combined datasets"""

import pandas as pd
from transformers import BertTokenizer
import torch
from torch.utils.data import Dataset

# Load the training and test datasets from Excel files
train_df = pd.read_excel("/content/ai_vs_human_text_cleaned.xlsx")
test_df = pd.read_excel("/content/test.xlsx")

# Extract texts and labels
train_texts = train_df['text'].tolist()
train_labels = train_df['label'].tolist()
test_texts = test_df['text'].tolist()
test_labels = test_df['label'].tolist()

# Load tokenizer (use uncased for lowercase data)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the data
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=512, return_tensors='pt')

# Dataset class
class TextDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Create datasets
train_dataset = TextDataset(train_encodings, train_labels)
test_dataset = TextDataset(test_encodings, test_labels)

print(f"Tokenization complete. Ready for model training.")
print(f"Train samples: {len(train_dataset)} | Test samples: {len(test_dataset)}")

import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import numpy as np
from transformers import BertForSequenceClassification, BertTokenizer
from torch.optim import AdamW

# Setup device
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
print("ðŸš€ Using device:", device)

# Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
model.to(device)

# Load Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Set up DataLoaders (assuming train_dataset and test_dataset are already defined)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(test_dataset, batch_size=128)

# Optimizer
optimizer = AdamW(model.parameters(), lr=2e-5)

# Training config
EPOCHS = 3
train_losses = []
val_losses = []
best_val_loss = float('inf')
best_epoch = -1
early_stop_patience = 3
patience_counter = 0

for epoch in range(EPOCHS):
    model.train()
    total_train_loss = 0
    print(f"\n... Epoch {epoch+1}/{EPOCHS}")
    loop = tqdm(train_loader, leave=False)

    for batch in loop:
        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch data to device
        outputs = model(**batch)  # Forward pass
        loss = outputs.loss
        loss.backward()  # Backpropagation
        optimizer.step()  # Optimizer step
        optimizer.zero_grad()  # Zero gradients
        total_train_loss += loss.item()  # Accumulate loss
        loop.set_description(f"Epoch {epoch+1}")
        loop.set_postfix(loss=loss.item())  # Update progress bar with loss

    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Validation
    model.eval()  # Set model to evaluation mode
    total_val_loss = 0
    with torch.no_grad():  # No gradients needed for validation
        for batch in val_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            loss = outputs.loss
            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)

    print(f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

    # Save current model after each epoch
    model_path = f"/content/model_epoch_{epoch+1}.pt"
    torch.save(model.state_dict(), model_path)

    # Best model tracking with early stopping
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        best_epoch = epoch + 1
        patience_counter = 0
        torch.save(model.state_dict(), "/content/best_model.pt")
    else:
        patience_counter += 1
        if patience_counter >= early_stop_patience:
            print(" ---- Early stopping triggered. ----")
            break

# Print the best epoch and its associated validation loss
print(f"\nBest model saved at epoch {best_epoch} with val loss {best_val_loss:.4f}")

# Plot training vs validation loss
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title("Training vs Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import numpy as np

# Load the best model
model.load_state_dict(torch.load("/content/best_model.pt"))
model.to(device)
model.eval()

all_preds = []
all_labels = []

with torch.no_grad():
    for batch in val_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=-1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch['labels'].cpu().numpy())

# Convert to numpy arrays
all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

# Metrics
accuracy = accuracy_score(all_labels, all_preds)
report = classification_report(all_labels, all_preds)
conf_matrix = confusion_matrix(all_labels, all_preds)

# Print evaluation results
print("Accuracy:", accuracy)
print("\n Classification Report:\n", report)
print("Confusion Matrix:\n", conf_matrix)

# Show 5 random predictions with their true labels
import random
idxs = random.sample(range(len(all_preds)), 5)
for i in idxs:
    print(f"True: {all_labels[i]}, Pred: {all_preds[i]}")

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 5))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["AI", "Human"], yticklabels=["AI", "Human"])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.show()

import torch
from transformers import BertTokenizer, BertForSequenceClassification
import torch.nn.functional as F

# Load the tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Load the saved model state dictionary (the weights) into the model
model.load_state_dict(torch.load('best_model.pt'))

# Ensure the model is in evaluation mode
model.eval()

# Define the function for text classification
def predict_text(input_text):
    # Tokenize the input text
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=512)

    # Move inputs to the same device as the model (if using GPU)
    inputs = {key: value.to(model.device) for key, value in inputs.items()}

    # Get the prediction
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits

        # Apply softmax to get probabilities
        probabilities = F.softmax(logits, dim=1)

        # Get the predicted class and its probability
        predicted_class = torch.argmax(probabilities, dim=1).item()
        confidence = probabilities[0][predicted_class].item() * 100  # Percentage confidence

    # Return the result based on the predicted class and its probability
    if predicted_class == 1:
        return f"Human-written (Real) - Confidence: {confidence:.2f}%"
    else:
        return f"AI-generated (Fake) - Confidence: {confidence:.2f}%"

# User Input Prompt
print("Welcome to TrueText, AI Text Classification Tool!\n")
print("Please enter the text to classify. When you're done, press Enter.\n")

# Read user input
user_input = input()

# Function to split long text into smaller chunks (e.g., 80 characters per line)
def split_into_lines(text, max_length=80):
    lines = []
    words = text.split()
    current_line = ""

    for word in words:
        # If adding the word exceeds the max length, start a new line
        if len(current_line) + len(word) + 1 > max_length:
            lines.append(current_line)
            current_line = word
        else:
            if current_line:
                current_line += " " + word
            else:
                current_line = word
    if current_line:
        lines.append(current_line)

    return "\n".join(lines)

# Display the user's input text with line breaks for readability
print("\nYou entered the following text:")
print("="*50)
print(split_into_lines(user_input))  # Split text into lines for better readability
print("="*50)

# Get the prediction
prediction = predict_text(user_input)

# Output the prediction
print("\nPrediction Result:")
print("="*50)
print(f"{prediction}")
print("="*50)

"""For  LSTM baseline...

"""

import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=128, num_classes=2, pad_idx=0):
        super(LSTMClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(0.3)
        self.fc = nn.Linear(hidden_dim * 2, num_classes)  # *2 for bidirectional

    def forward(self, input_ids, attention_mask=None, labels=None):
        embedded = self.embedding(input_ids)
        lstm_out, _ = self.lstm(embedded)
        pooled = torch.mean(lstm_out, dim=1)  # Mean pooling over time
        dropped = self.dropout(pooled)
        logits = self.fc(dropped)

        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)

        return {"loss": loss, "logits": logits}

from torch.utils.data import Dataset
from sklearn.model_selection import train_test_split

# Define a custom dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encodings = self.tokenizer.encode_plus(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        item = {
            'input_ids': encodings['input_ids'].squeeze(0),  # remove batch dimension
            'labels': torch.tensor(self.labels[idx], dtype=torch.long)
        }
        return item

# Load your cleaned dataset
df = pd.read_csv('/content/ai_vs_human_text_cleaned.csv')

# Split data
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'].tolist(),
    df['label'].tolist(),
    test_size=0.2,
    random_state=42
)

# Use the same tokenizer you used before
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Create datasets
train_dataset = TextDataset(train_texts, train_labels, tokenizer)
val_dataset = TextDataset(val_texts, val_labels, tokenizer)

import torch
from torch.utils.data import DataLoader
from torch.optim import Adam
from tqdm import tqdm
import matplotlib.pyplot as plt

# Hyperparameters
vocab_size = tokenizer.vocab_size  # Use the same tokenizer from your BERT model
embedding_dim = 128
hidden_dim = 128

# Model
model = LSTMClassifier(vocab_size=vocab_size, embedding_dim=embedding_dim, hidden_dim=hidden_dim)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Dataloaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)

# Optimizer
optimizer = Adam(model.parameters(), lr=2e-4)

# Lists to store loss values for plotting
train_losses = []
val_losses = []

# Best validation loss tracking
best_val_loss = float('inf')
best_model_path = "best_lstm_model.pth"

# Training loop
EPOCHS = 25
for epoch in range(EPOCHS):
    model.train()
    total_train_loss = 0
    print(f"\nEpoch {epoch+1}/{EPOCHS}")

    for batch in tqdm(train_loader):
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()  # Reset gradients before backward pass
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs['loss']
        loss.backward()  # Backpropagate
        optimizer.step()  # Update weights

        total_train_loss += loss.item()

    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)

    # Validation loop
    model.eval()
    total_val_loss = 0
    correct = 0
    total = 0
    with torch.no_grad():  # Disable gradients for validation phase
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            outputs = model(input_ids=input_ids, labels=labels)
            logits = outputs['logits']
            loss = outputs['loss']
            total_val_loss += loss.item()

            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)
    val_accuracy = correct / total

    print(f"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val Acc: {val_accuracy:.4f}")

    # Save the best model based on the lowest validation loss
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(model.state_dict(), best_model_path)
        print(f" Saved the best model with Val Loss: {avg_val_loss:.4f}")

# Plotting training and validation loss
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.title("Training vs Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

print(f"Best model saved at {best_model_path}")

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np

all_preds = []
all_labels = []

model.eval()
with torch.no_grad():
    for batch in val_loader:
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids=input_ids)
        logits = outputs['logits']
        preds = torch.argmax(logits, dim=1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

# Convert to NumPy arrays
all_preds = np.array(all_preds)
all_labels = np.array(all_labels)

# Metrics
accuracy = accuracy_score(all_labels, all_preds)
conf_matrix = confusion_matrix(all_labels, all_preds)
report = classification_report(all_labels, all_preds)

# Display results
print(f" Accuracy: {accuracy:.4f}")
print("\nClassification Report:\n", report)
print(" Confusion Matrix:\n", conf_matrix)